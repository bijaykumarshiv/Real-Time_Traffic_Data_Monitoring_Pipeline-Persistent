{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d27dc6-a8b7-4223-9d8a-1eaeffc83cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# AUTHENTICATION FOR S3 ACCESS\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA3T4PJQNLDBEON3PW\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-north-1.amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c363f0-75c9-4b1c-8224-07a5ef0701f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"USE traffic_db\") # use database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c392f928-455e-4119-a7a3-29cde7d83f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  SILVER TRANSFORMATION LAYER               \n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE traffic_db\")\n",
    "\n",
    "# 1. Load Bronze Table\n",
    "bronze_df = spark.table(\"traffic_bronze\")\n",
    "display(bronze_df.limit(10))\n",
    "\n",
    "\n",
    "# --- Standard cleaning ---\n",
    "silver_df = bronze_df \\\n",
    "    .dropDuplicates() \\\n",
    "    .dropna(subset=[\"avg_speed_kmph\", \"density_veh_per_km\", \"road_segment_id\"])  #Remove rows with critical null fld \n",
    "\n",
    "# --- Convert numeric columns ---\n",
    "numeric_columns = [\"avg_speed_kmph\", \"density_veh_per_km\", \"avg_wait_time_s\",\n",
    "                   \"occupancy_pct\", \"flow_veh_per_hr\"]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    silver_df = silver_df.withColumn(col, F.col(col).cast(\"float\"))\n",
    "\n",
    "\n",
    "# --- Remove Invalid Data / Outliers ---\n",
    "silver_df = silver_df.filter(\n",
    "    (F.col(\"avg_speed_kmph\") > 0) & (F.col(\"avg_speed_kmph\") < 200) &         # unrealistic speed removal\n",
    "    (F.col(\"density_veh_per_km\") >= 0) &\n",
    "    (F.col(\"occupancy_pct\") >= 0) & (F.col(\"occupancy_pct\") <= 100)           # valid percentage range\n",
    ")\n",
    "\n",
    "\n",
    "# --- Deduplicate based on road,time pair ---\n",
    "silver_df = silver_df.dropDuplicates([\"road_segment_id\", \"event_time\"])\n",
    "\n",
    "\n",
    "# --- Handle timestamp ---\n",
    "if \"timestamp\" in bronze_df.columns:\n",
    "    silver_df = silver_df.withColumn(\"event_time\", F.to_timestamp(\"timestamp\"))\n",
    "else:\n",
    "    silver_df = silver_df.withColumn(\"event_time\", F.current_timestamp())\n",
    "\n",
    "display(silver_df.limit(20))\n",
    "\n",
    "\n",
    "# 3. Write Silver to S3\n",
    "silver_path = \"s3://traffic-data-monitoring-project/silver/traffic_cleaned/\"\n",
    "\n",
    "(silver_df.write\n",
    "         .mode(\"overwrite\")\n",
    "         .format(\"delta\")\n",
    "         .save(silver_path)\n",
    ")\n",
    "\n",
    "# 4. Register table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS traffic_silver\n",
    "USING DELTA\n",
    "LOCATION '{silver_path}'\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"traffic_silver\").limit(20))\n",
    "print(\"\uD83E\uDD48 SILVER LAYER SUCCESSFULLY CREATED!\")\n",
    "print(\"âœ” Silver job completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}